{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine_Tune_BERT_SQuAD_2_0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIH-A1BE-9lP"
      },
      "source": [
        "### <b> 1. Install some Libraries </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvXZTa8doh_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab272bf-2f4b-4d95-98ec-37709cc0ec07"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb  5 17:45:39 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkuwFEpiotU3",
        "outputId": "b6af7dc5-3e95-45f6-87ab-ff10c7dc7791"
      },
      "source": [
        "from psutil import virtual_memory\r\n",
        "ram_gb = virtual_memory().total / 1e9\r\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\r\n",
        "\r\n",
        "if ram_gb < 20:\r\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\r\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\r\n",
        "  print('re-execute this cell.')\r\n",
        "else:\r\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgPiOQbm-1kx"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install datasets\r\n",
        "!pip install nlp\r\n",
        "!pip install git+git://github.com/huggingface/transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5hMbaG318QA",
        "outputId": "8c44f33c-213b-48f2-9994-5d12c0d52e13"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)\r\n",
        "root_dir = '/content/gdrive/My Drive/Colab Notebooks/NLP_Portfolio/Question_Answering_on_SQuAD_2_0/' # Change it to your own directory\r\n",
        "\r\n",
        "import os\r\n",
        "os.chdir(root_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoCcntD-zRTd"
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "import sys\r\n",
        "from dataclasses import dataclass, field\r\n",
        "from typing import Optional\r\n",
        "\r\n",
        "from datasets import load_dataset, load_metric\r\n",
        "\r\n",
        "import transformers\r\n",
        "from trainer_qa import QuestionAnsweringTrainer\r\n",
        "from transformers import (\r\n",
        "    AutoConfig,\r\n",
        "    AutoModelForQuestionAnswering,\r\n",
        "    AutoTokenizer,\r\n",
        "    DataCollatorWithPadding,\r\n",
        "    EvalPrediction,\r\n",
        "    HfArgumentParser,\r\n",
        "    PreTrainedTokenizerFast,\r\n",
        "    TrainingArguments,\r\n",
        "    default_data_collator,\r\n",
        "    set_seed,\r\n",
        ")\r\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\r\n",
        "from utils_qa import postprocess_qa_predictions\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENlJIukniaPz"
      },
      "source": [
        "### <b> 2. Define the Arguments </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5fBAyov2P8Y"
      },
      "source": [
        "@dataclass\r\n",
        "class ModelArguments:\r\n",
        "    \"\"\"\r\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    model_name_or_path: str = field(\r\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\r\n",
        "    )\r\n",
        "    config_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\r\n",
        "    )\r\n",
        "    tokenizer_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\r\n",
        "    )\r\n",
        "    cache_dir: Optional[str] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\r\n",
        "    )\r\n",
        "    model_revision: str = field(\r\n",
        "        default=\"main\",\r\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\r\n",
        "    )\r\n",
        "    use_auth_token: bool = field(\r\n",
        "        default=False,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\r\n",
        "            \"with private models).\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class DataTrainingArguments:\r\n",
        "    \"\"\"\r\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    dataset_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\r\n",
        "    )\r\n",
        "    dataset_config_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\r\n",
        "    )\r\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\r\n",
        "    validation_file: Optional[str] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\r\n",
        "    )\r\n",
        "    overwrite_cache: bool = field(\r\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\r\n",
        "    )\r\n",
        "    preprocessing_num_workers: Optional[int] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\r\n",
        "    )\r\n",
        "    max_seq_length: int = field(\r\n",
        "        default=384,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\r\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    pad_to_max_length: bool = field(\r\n",
        "        default=True,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\r\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch (which can \"\r\n",
        "            \"be faster on GPU but will be slower on TPU).\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    version_2_with_negative: bool = field(\r\n",
        "        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\r\n",
        "    )\r\n",
        "    null_score_diff_threshold: float = field(\r\n",
        "        default=0.0,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The threshold used to select the null answer: if the best answer has a score that is less than \"\r\n",
        "            \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\r\n",
        "            \"Only useful when `version_2_with_negative=True`.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    doc_stride: int = field(\r\n",
        "        default=128,\r\n",
        "        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\r\n",
        "    )\r\n",
        "    n_best_size: int = field(\r\n",
        "        default=20,\r\n",
        "        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\r\n",
        "    )\r\n",
        "    max_answer_length: int = field(\r\n",
        "        default=30,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The maximum length of an answer that can be generated. This is needed because the start \"\r\n",
        "            \"and end predictions are not conditioned on one another.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "\r\n",
        "    def __post_init__(self):\r\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\r\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\r\n",
        "        else:\r\n",
        "            if self.train_file is not None:\r\n",
        "                extension = self.train_file.split(\".\")[-1]\r\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\r\n",
        "            if self.validation_file is not None:\r\n",
        "                extension = self.validation_file.split(\".\")[-1]\r\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8xu0VgZqm4"
      },
      "source": [
        "args_dict = {\r\n",
        "  \"n_gpu\": 1,\r\n",
        "  \"model_name_or_path\": 'bert-base-uncased',\r\n",
        "  \"dataset_name\": 'squad_v2',\r\n",
        "  \"version_2_with_negative\": True,\r\n",
        "  \"max_seq_length\": 384,\r\n",
        "  \"output_dir\": './models',\r\n",
        "  \"overwrite_output_dir\": True, # True: Fine-tune from scratch, False: Continue fine-tuning  \r\n",
        "  \"per_device_train_batch_size\": 12,\r\n",
        "  \"per_device_eval_batch_size\": 12,\r\n",
        "  \"learning_rate\": 3e-5,\r\n",
        "  \"num_train_epochs\": 2,\r\n",
        "  \"doc_stride\": 128,\r\n",
        "  \"do_train\": True,\r\n",
        "  \"do_eval\": True,\r\n",
        "  \"save_steps\": 5000,\r\n",
        "  \"logging_steps\": 5000\r\n",
        "}\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "with open(root_dir + 'args.json', 'w') as f:\r\n",
        "  json.dump(args_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d0bkgYYihiI"
      },
      "source": [
        "### <b> 3. Main </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtR5e7Aa2QHq"
      },
      "source": [
        "def main():\r\n",
        "    # See all possible arguments in src/transformers/training_args.py at Huggingface Transformers repo \r\n",
        "    # or by passing the --help flag to this script.\r\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\r\n",
        "\r\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\r\n",
        "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\r\n",
        "\r\n",
        "    # Detecting last checkpoint.\r\n",
        "    last_checkpoint = None\r\n",
        "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\r\n",
        "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\r\n",
        "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\r\n",
        "            raise ValueError(\r\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\r\n",
        "                \"Use --overwrite_output_dir to overcome.\"\r\n",
        "            )\r\n",
        "        elif last_checkpoint is not None:\r\n",
        "            logger.info(\r\n",
        "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\r\n",
        "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\r\n",
        "            )\r\n",
        "\r\n",
        "    # Setup logging\r\n",
        "    logging.basicConfig(\r\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\r\n",
        "    )\r\n",
        "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\r\n",
        "\r\n",
        "    # Log on each process the small summary:\r\n",
        "    logger.warning(\r\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\r\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\r\n",
        "    )\r\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\r\n",
        "    if is_main_process(training_args.local_rank):\r\n",
        "        transformers.utils.logging.set_verbosity_info()\r\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\r\n",
        "\r\n",
        "    # Set seed before initializing model.\r\n",
        "    set_seed(training_args.seed)\r\n",
        "\r\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\r\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\r\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\r\n",
        "    #\r\n",
        "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\r\n",
        "    # 'text' is found. You can easily tweak this behavior (see below).\r\n",
        "    #\r\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\r\n",
        "    # download the dataset.\r\n",
        "    if data_args.dataset_name is not None:\r\n",
        "        # Downloading and loading a dataset from the hub.\r\n",
        "        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n",
        "    else:\r\n",
        "        data_files = {}\r\n",
        "        if data_args.train_file is not None:\r\n",
        "            data_files[\"train\"] = data_args.train_file\r\n",
        "        if data_args.validation_file is not None:\r\n",
        "            data_files[\"validation\"] = data_args.validation_file\r\n",
        "        extension = data_args.train_file.split(\".\")[-1]\r\n",
        "        datasets = load_dataset(extension, data_files=data_files, field=\"data\")\r\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\r\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\r\n",
        "\r\n",
        "    # Load pretrained model and tokenizer\r\n",
        "    #\r\n",
        "    # Distributed training:\r\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\r\n",
        "    # download model & vocab.\r\n",
        "    config = AutoConfig.from_pretrained(\r\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\r\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        use_fast=True,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\r\n",
        "        model_args.model_name_or_path,\r\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\r\n",
        "        config=config,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Tokenizer check: this script requires a fast tokenizer.\r\n",
        "    if not isinstance(tokenizer, PreTrainedTokenizerFast):\r\n",
        "        raise ValueError(\r\n",
        "            \"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"\r\n",
        "            \"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"\r\n",
        "            \"requirement\"\r\n",
        "        )\r\n",
        "\r\n",
        "    # Preprocessing the datasets.\r\n",
        "    # Preprocessing is slighlty different for training and evaluation.\r\n",
        "    if training_args.do_train:\r\n",
        "        column_names = datasets[\"train\"].column_names\r\n",
        "    else:\r\n",
        "        column_names = datasets[\"validation\"].column_names\r\n",
        "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\r\n",
        "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\r\n",
        "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\r\n",
        "\r\n",
        "    # Padding side determines if we do (question|context) or (context|question).\r\n",
        "    pad_on_right = tokenizer.padding_side == \"right\"\r\n",
        "\r\n",
        "    # Training preprocessing\r\n",
        "    def prepare_train_features(examples):\r\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\r\n",
        "        # context that overlaps a bit the context of the previous feature.\r\n",
        "        tokenized_examples = tokenizer(\r\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\r\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\r\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n",
        "            max_length=data_args.max_seq_length,\r\n",
        "            stride=data_args.doc_stride,\r\n",
        "            return_overflowing_tokens=True,\r\n",
        "            return_offsets_mapping=True,\r\n",
        "            padding=\"max_length\" if data_args.pad_to_max_length else False,\r\n",
        "        )\r\n",
        "\r\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n",
        "        # its corresponding example. This key gives us just that.\r\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\r\n",
        "        # help us compute the start_positions and end_positions.\r\n",
        "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\r\n",
        "\r\n",
        "        # Let's label those examples!\r\n",
        "        tokenized_examples[\"start_positions\"] = []\r\n",
        "        tokenized_examples[\"end_positions\"] = []\r\n",
        "        tokenized_examples[\"is_impossible\"] = [] \r\n",
        "        tokenized_examples[\"cls_index\"] = [] \r\n",
        "\r\n",
        "        for i, offsets in enumerate(offset_mapping):\r\n",
        "            # We will label impossible answers with the index of the CLS token.\r\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\r\n",
        "            cls_index = input_ids.index(tokenizer.cls_token_id)\r\n",
        "            tokenized_examples[\"cls_index\"].append(cls_index) \r\n",
        "\r\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\r\n",
        "\r\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\r\n",
        "            sample_index = sample_mapping[i]\r\n",
        "            answers = examples[answer_column_name][sample_index]\r\n",
        "            # If no answers are given, set the cls_index as answer.\r\n",
        "            if len(answers[\"answer_start\"]) == 0:\r\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\r\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\r\n",
        "                tokenized_examples[\"is_impossible\"].append(1.0) \r\n",
        "            else:\r\n",
        "                # Start/end character index of the answer in the text.\r\n",
        "                start_char = answers[\"answer_start\"][0]\r\n",
        "                end_char = start_char + len(answers[\"text\"][0])\r\n",
        "\r\n",
        "                # Start token index of the current span in the text.\r\n",
        "                token_start_index = 0\r\n",
        "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\r\n",
        "                    token_start_index += 1\r\n",
        "\r\n",
        "                # End token index of the current span in the text.\r\n",
        "                token_end_index = len(input_ids) - 1\r\n",
        "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\r\n",
        "                    token_end_index -= 1\r\n",
        "\r\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\r\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\r\n",
        "                    tokenized_examples[\"start_positions\"].append(cls_index)\r\n",
        "                    tokenized_examples[\"end_positions\"].append(cls_index)\r\n",
        "                    tokenized_examples[\"is_impossible\"].append(1.0) \r\n",
        "                else:\r\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\r\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\r\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\r\n",
        "                        token_start_index += 1\r\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\r\n",
        "                    while offsets[token_end_index][1] >= end_char:\r\n",
        "                        token_end_index -= 1\r\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\r\n",
        "                    tokenized_examples[\"is_impossible\"].append(0.0)\r\n",
        "\r\n",
        "        return tokenized_examples\r\n",
        "\r\n",
        "    if training_args.do_train:\r\n",
        "        train_dataset = datasets[\"train\"].map(\r\n",
        "            prepare_train_features,\r\n",
        "            batched=True,\r\n",
        "            num_proc=data_args.preprocessing_num_workers,\r\n",
        "            remove_columns=column_names,\r\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Validation preprocessing\r\n",
        "    def prepare_validation_features(examples):\r\n",
        "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\r\n",
        "        # context that overlaps a bit the context of the previous feature.\r\n",
        "        tokenized_examples = tokenizer(\r\n",
        "            examples[question_column_name if pad_on_right else context_column_name],\r\n",
        "            examples[context_column_name if pad_on_right else question_column_name],\r\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n",
        "            max_length=data_args.max_seq_length,\r\n",
        "            stride=data_args.doc_stride,\r\n",
        "            return_overflowing_tokens=True,\r\n",
        "            return_offsets_mapping=True,\r\n",
        "            padding=\"max_length\" if data_args.pad_to_max_length else False,\r\n",
        "        )\r\n",
        "\r\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n",
        "        # its corresponding example. This key gives us just that.\r\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n",
        "\r\n",
        "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\r\n",
        "        # corresponding example_id and we will store the offset mappings.\r\n",
        "        tokenized_examples[\"example_id\"] = []\r\n",
        "\r\n",
        "        # We still provide the index of the CLS token, but not the is_impossible label. \r\n",
        "        tokenized_examples[\"cls_index\"] = [] \r\n",
        "\r\n",
        "        for i in range(len(tokenized_examples[\"input_ids\"])):\r\n",
        "            # Find the CLS token in the input ids. \r\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\r\n",
        "            cls_index = input_ids.index(tokenizer.cls_token_id)\r\n",
        "            tokenized_examples[\"cls_index\"].append(cls_index) \r\n",
        "\r\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\r\n",
        "            context_index = 1 if pad_on_right else 0\r\n",
        "\r\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\r\n",
        "            sample_index = sample_mapping[i]\r\n",
        "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n",
        "\r\n",
        "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\r\n",
        "            # position is part of the context or not.\r\n",
        "            tokenized_examples[\"offset_mapping\"][i] = [\r\n",
        "                (o if sequence_ids[k] == context_index else None)\r\n",
        "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n",
        "            ]\r\n",
        "\r\n",
        "        return tokenized_examples\r\n",
        "\r\n",
        "    if training_args.do_eval:\r\n",
        "        validation_dataset = datasets[\"validation\"].map(\r\n",
        "            prepare_validation_features,\r\n",
        "            batched=True,\r\n",
        "            num_proc=data_args.preprocessing_num_workers,\r\n",
        "            remove_columns=column_names,\r\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\r\n",
        "        )\r\n",
        "\r\n",
        "    # Data collator\r\n",
        "    # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\r\n",
        "    # collator.\r\n",
        "    data_collator = (\r\n",
        "        default_data_collator\r\n",
        "        if data_args.pad_to_max_length\r\n",
        "        else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\r\n",
        "    )\r\n",
        "\r\n",
        "    # Post-processing:\r\n",
        "    def post_processing_function(examples, features, predictions):\r\n",
        "        # Post-processing: we match the start logits and end logits to answers in the original context.\r\n",
        "        predictions = postprocess_qa_predictions(\r\n",
        "            examples=examples,\r\n",
        "            features=features,\r\n",
        "            predictions=predictions,\r\n",
        "            version_2_with_negative=data_args.version_2_with_negative,\r\n",
        "            n_best_size=data_args.n_best_size,\r\n",
        "            max_answer_length=data_args.max_answer_length,\r\n",
        "            null_score_diff_threshold=data_args.null_score_diff_threshold,\r\n",
        "            output_dir=training_args.output_dir,\r\n",
        "            is_world_process_zero=trainer.is_world_process_zero(),\r\n",
        "        )\r\n",
        "        # Format the result to the format the metric expects.\r\n",
        "        if data_args.version_2_with_negative:\r\n",
        "            formatted_predictions = [\r\n",
        "                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\r\n",
        "            ]\r\n",
        "        else:\r\n",
        "            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\r\n",
        "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in datasets[\"validation\"]]\r\n",
        "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\r\n",
        "\r\n",
        "    metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\r\n",
        "\r\n",
        "    def compute_metrics(p: EvalPrediction):\r\n",
        "        return metric.compute(predictions=p.predictions, references=p.label_ids)\r\n",
        "\r\n",
        "    # Initialize our Trainer\r\n",
        "    trainer = QuestionAnsweringTrainer(\r\n",
        "        model=model,\r\n",
        "        args=training_args,\r\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\r\n",
        "        eval_dataset=validation_dataset if training_args.do_eval else None,\r\n",
        "        eval_examples=datasets[\"validation\"] if training_args.do_eval else None,\r\n",
        "        tokenizer=tokenizer,\r\n",
        "        data_collator=data_collator,\r\n",
        "        post_process_function=post_processing_function,\r\n",
        "        compute_metrics=compute_metrics,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Training\r\n",
        "    if training_args.do_train:\r\n",
        "        if last_checkpoint is not None:\r\n",
        "            checkpoint = last_checkpoint\r\n",
        "        elif os.path.isdir(model_args.model_name_or_path):\r\n",
        "            checkpoint = model_args.model_name_or_path\r\n",
        "        else:\r\n",
        "            checkpoint = None\r\n",
        "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\r\n",
        "        trainer.save_model()  # Saves the tokenizer too for easy upload\r\n",
        "\r\n",
        "        output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\r\n",
        "        if trainer.is_world_process_zero():\r\n",
        "            with open(output_train_file, \"w\") as writer:\r\n",
        "                logger.info(\"***** Train results *****\")\r\n",
        "                for key, value in sorted(train_result.metrics.items()):\r\n",
        "                    logger.info(f\"  {key} = {value}\")\r\n",
        "                    writer.write(f\"{key} = {value}\\n\")\r\n",
        "\r\n",
        "            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\r\n",
        "            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\r\n",
        "\r\n",
        "    # Evaluation\r\n",
        "    results = {}\r\n",
        "    if training_args.do_eval:\r\n",
        "        logger.info(\"*** Evaluate ***\")\r\n",
        "        results = trainer.evaluate()\r\n",
        "\r\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\r\n",
        "        if trainer.is_world_process_zero():\r\n",
        "            with open(output_eval_file, \"w\") as writer:\r\n",
        "                logger.info(\"***** Eval results *****\")\r\n",
        "                for key, value in sorted(results.items()):\r\n",
        "                    logger.info(f\"  {key} = {value}\")\r\n",
        "                    writer.write(f\"{key} = {value}\\n\")\r\n",
        "\r\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FooNjlSe2QQ6"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
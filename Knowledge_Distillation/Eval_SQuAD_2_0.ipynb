{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eval_SQuAD_2_0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIH-A1BE-9lP"
      },
      "source": [
        "### <b> 1. Install some Libraries </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvXZTa8doh_h",
        "outputId": "0ac5b9f6-bc10-498e-9d57-628f647e00ea"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 12 22:10:13 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkuwFEpiotU3",
        "outputId": "9ada56f9-a23a-4014-c4c2-21af488ef35b"
      },
      "source": [
        "from psutil import virtual_memory\r\n",
        "ram_gb = virtual_memory().total / 1e9\r\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\r\n",
        "\r\n",
        "if ram_gb < 20:\r\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\r\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\r\n",
        "  print('re-execute this cell.')\r\n",
        "else:\r\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgPiOQbm-1kx"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install datasets\r\n",
        "!pip install nlp\r\n",
        "!pip install git+git://github.com/huggingface/transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5hMbaG318QA"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)\r\n",
        "root_dir = '/content/gdrive/My Drive/Colab Notebooks/NLP_Portfolio/Question_Answering_on_SQuAD_2_0/Knowledge_Distillation'\r\n",
        "\r\n",
        "import os\r\n",
        "os.chdir(root_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5rt-fRKGRK"
      },
      "source": [
        "import transformers\r\n",
        "from trainer_qa import QuestionAnsweringTrainer\r\n",
        "from transformers import (\r\n",
        "    DistilBertConfig,\r\n",
        "    DistilBertForQuestionAnswering,\r\n",
        "    DistilBertTokenizer,\r\n",
        "    AutoTokenizer,\r\n",
        "    DataCollatorWithPadding,\r\n",
        "    EvalPrediction,\r\n",
        "    HfArgumentParser,\r\n",
        "    PreTrainedTokenizerFast,\r\n",
        "    TrainingArguments,\r\n",
        "    default_data_collator,\r\n",
        "    set_seed,\r\n",
        ")\r\n",
        "\r\n",
        "from typing import Optional\r\n",
        "from dataclasses import dataclass, field\r\n",
        "from datasets import load_dataset, load_metric\r\n",
        "from utils_qa import postprocess_qa_predictions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrpCFk84dZ_m"
      },
      "source": [
        "### <b> 2. Define the Arguments </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uSRq-nSKQFr"
      },
      "source": [
        "args_dict = {\r\n",
        "  \"n_gpu\": 1,\r\n",
        "  # Use \"./models_without_distillation/pytorch_model.bin\" if you want to evaluate the DistilBert which is fine-tuned without distillation\r\n",
        "  \"model_name_or_path\": './models_with_distillation/pytorch_model.bin',\r\n",
        "  \"dataset_name\": 'squad_v2',\r\n",
        "  \"version_2_with_negative\": True,\r\n",
        "  \"max_seq_length\": 384,\r\n",
        "  \"output_dir\": './models',\r\n",
        "  \"overwrite_output_dir\": True,\r\n",
        "  \"per_device_eval_batch_size\": 8,\r\n",
        "  \"doc_stride\": 128,\r\n",
        "  \"do_eval\": True,\r\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv_lNK6wOMX0"
      },
      "source": [
        "@dataclass\r\n",
        "class ModelArguments:\r\n",
        "    \"\"\"\r\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    model_name_or_path: str = field(\r\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\r\n",
        "    )\r\n",
        "    config_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\r\n",
        "    )\r\n",
        "    tokenizer_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\r\n",
        "    )\r\n",
        "    cache_dir: Optional[str] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\r\n",
        "    )\r\n",
        "    model_revision: str = field(\r\n",
        "        default=\"main\",\r\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\r\n",
        "    )\r\n",
        "    use_auth_token: bool = field(\r\n",
        "        default=False,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\r\n",
        "            \"with private models).\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class DataTrainingArguments:\r\n",
        "    \"\"\"\r\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    dataset_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\r\n",
        "    )\r\n",
        "    dataset_config_name: Optional[str] = field(\r\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\r\n",
        "    )\r\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\r\n",
        "    validation_file: Optional[str] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\r\n",
        "    )\r\n",
        "    overwrite_cache: bool = field(\r\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\r\n",
        "    )\r\n",
        "    preprocessing_num_workers: Optional[int] = field(\r\n",
        "        default=None,\r\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\r\n",
        "    )\r\n",
        "    max_seq_length: int = field(\r\n",
        "        default=384,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\r\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    pad_to_max_length: bool = field(\r\n",
        "        default=True,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\r\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch (which can \"\r\n",
        "            \"be faster on GPU but will be slower on TPU).\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    version_2_with_negative: bool = field(\r\n",
        "        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\r\n",
        "    )\r\n",
        "    null_score_diff_threshold: float = field(\r\n",
        "        default=0.0,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The threshold used to select the null answer: if the best answer has a score that is less than \"\r\n",
        "            \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\r\n",
        "            \"Only useful when `version_2_with_negative=True`.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "    doc_stride: int = field(\r\n",
        "        default=128,\r\n",
        "        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\r\n",
        "    )\r\n",
        "    n_best_size: int = field(\r\n",
        "        default=20,\r\n",
        "        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\r\n",
        "    )\r\n",
        "    max_answer_length: int = field(\r\n",
        "        default=30,\r\n",
        "        metadata={\r\n",
        "            \"help\": \"The maximum length of an answer that can be generated. This is needed because the start \"\r\n",
        "            \"and end predictions are not conditioned on one another.\"\r\n",
        "        },\r\n",
        "    )\r\n",
        "\r\n",
        "    def __post_init__(self):\r\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\r\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\r\n",
        "        else:\r\n",
        "            if self.train_file is not None:\r\n",
        "                extension = self.train_file.split(\".\")[-1]\r\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\r\n",
        "            if self.validation_file is not None:\r\n",
        "                extension = self.validation_file.split(\".\")[-1]\r\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnhTmiSId1aI"
      },
      "source": [
        "### <b> 3. Load the tokenizer and the DistilBERT model fine-tuned on SQuAD 2.0 </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0U86dOgOTgy"
      },
      "source": [
        "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\r\n",
        "model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('models_with_distillation/args.json')) # or 'models_without_distillation/args.json'\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\r\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        use_fast=True,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "config = DistilBertConfig.from_pretrained(\r\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\r\n",
        "        model_args.model_name_or_path,\r\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\r\n",
        "        config=config,\r\n",
        "        cache_dir=model_args.cache_dir,\r\n",
        "        revision=model_args.model_revision,\r\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\r\n",
        "    )\r\n",
        "\r\n",
        "import torch\r\n",
        "model.load_state_dict(torch.load(args_dict[\"model_name_or_path\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1lJBs6eeeTq"
      },
      "source": [
        "### <b> 4. Load the metrics and convert the logits to answers in the original context </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JTR--s0SaY8"
      },
      "source": [
        "# Post-processing:\r\n",
        "def post_processing_function(examples, features, predictions):\r\n",
        "    # Post-processing: we match the start logits and end logits to answers in the original context.\r\n",
        "    predictions = postprocess_qa_predictions(\r\n",
        "        examples=examples,\r\n",
        "        features=features,\r\n",
        "        predictions=predictions,\r\n",
        "        version_2_with_negative=data_args.version_2_with_negative,\r\n",
        "        n_best_size=data_args.n_best_size,\r\n",
        "        max_answer_length=data_args.max_answer_length,\r\n",
        "        null_score_diff_threshold=data_args.null_score_diff_threshold,\r\n",
        "        output_dir=training_args.output_dir,\r\n",
        "        is_world_process_zero=trainer.is_world_process_zero(),\r\n",
        "    )\r\n",
        "    # Format the result to the format the metric expects.\r\n",
        "    if data_args.version_2_with_negative:\r\n",
        "        formatted_predictions = [\r\n",
        "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\r\n",
        "        ]\r\n",
        "    else:\r\n",
        "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\r\n",
        "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in datasets[\"validation\"]]\r\n",
        "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\r\n",
        "\r\n",
        "metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\r\n",
        "\r\n",
        "def compute_metrics(p: EvalPrediction):\r\n",
        "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kce8w5nxe0m_"
      },
      "source": [
        "### <b> 5. Import the Dataset </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWlKY9l2Nzcp"
      },
      "source": [
        "datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzCcQN3lR6Yk",
        "outputId": "e0967ed7-144c-47aa-9c39-22190bc462f2"
      },
      "source": [
        "datasets['train'][0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [269], 'text': ['in the late 1990s']},\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'id': '56be85543aeaaa14008c9063',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'title': 'Beyoncé'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ7xQQXpSPWd",
        "outputId": "a0a0eadf-506b-4184-8a30-3960a6e79d58"
      },
      "source": [
        "column_names = datasets[\"validation\"].column_names\r\n",
        "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\r\n",
        "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\r\n",
        "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\r\n",
        "\r\n",
        "datasets['validation'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [159, 159, 159, 159],\n",
              "  'text': ['France', 'France', 'France', 'France']},\n",
              " 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
              " 'id': '56ddde6b9a695914005b9628',\n",
              " 'question': 'In what country is Normandy located?',\n",
              " 'title': 'Normans'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRUrGqETfHrd"
      },
      "source": [
        "### <b> 6. Pre-process the Validation data </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZDePCZkTXYO"
      },
      "source": [
        "# Padding side determines if we do (question|context) or (context|question).\r\n",
        "pad_on_right = tokenizer.padding_side == \"right\"\r\n",
        "\r\n",
        "def prepare_validation_features(examples):\r\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\r\n",
        "    # context that overlaps a bit the context of the previous feature.\r\n",
        "    tokenized_examples = tokenizer(\r\n",
        "        examples[question_column_name if pad_on_right else context_column_name],\r\n",
        "        examples[context_column_name if pad_on_right else question_column_name],\r\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n",
        "        max_length=data_args.max_seq_length,\r\n",
        "        stride=data_args.doc_stride,\r\n",
        "        return_overflowing_tokens=True,\r\n",
        "        return_offsets_mapping=True,\r\n",
        "        padding=\"max_length\" if data_args.pad_to_max_length else False,\r\n",
        "    )\r\n",
        "\r\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n",
        "    # its corresponding example. This key gives us just that.\r\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n",
        "\r\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\r\n",
        "    # corresponding example_id and we will store the offset mappings.\r\n",
        "    tokenized_examples[\"example_id\"] = []\r\n",
        "\r\n",
        "    # We still provide the index of the CLS token, but not the is_impossible label. \r\n",
        "    tokenized_examples[\"cls_index\"] = [] \r\n",
        "\r\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\r\n",
        "        # Find the CLS token in the input ids. \r\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\r\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id) \r\n",
        "        tokenized_examples[\"cls_index\"].append(cls_index) \r\n",
        "\r\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\r\n",
        "        context_index = 1 if pad_on_right else 0\r\n",
        "\r\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\r\n",
        "        sample_index = sample_mapping[i]\r\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n",
        "\r\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\r\n",
        "        # position is part of the context or not.\r\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\r\n",
        "            (o if sequence_ids[k] == context_index else None)\r\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n",
        "        ]\r\n",
        "\r\n",
        "    return tokenized_examples"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BSj6GtXR3m-"
      },
      "source": [
        "if training_args.do_eval:\r\n",
        "    validation_dataset = datasets[\"validation\"].map(\r\n",
        "        prepare_validation_features,\r\n",
        "        batched=True,\r\n",
        "        num_proc=data_args.preprocessing_num_workers,\r\n",
        "        remove_columns=column_names,\r\n",
        "        load_from_cache_file=not data_args.overwrite_cache,\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NiCwepzhtW6"
      },
      "source": [
        "### <b> 7. Evaluation </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTjxMztQfov"
      },
      "source": [
        "# Data collator\r\n",
        "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\r\n",
        "# collator.\r\n",
        "data_collator = (\r\n",
        "    default_data_collator\r\n",
        "    if data_args.pad_to_max_length\r\n",
        "    else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\r\n",
        ")\r\n",
        "\r\n",
        "# Initialize our Trainer\r\n",
        "trainer = QuestionAnsweringTrainer(\r\n",
        "    model=model,\r\n",
        "    args=training_args,\r\n",
        "    train_dataset=None, # Because we only want to evaluate our model on Validation data\r\n",
        "    eval_dataset=validation_dataset if training_args.do_eval else None,\r\n",
        "    eval_examples=datasets[\"validation\"] if training_args.do_eval else None,\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    data_collator=data_collator,\r\n",
        "    post_process_function=post_processing_function,\r\n",
        "    compute_metrics=compute_metrics\r\n",
        ")\r\n",
        "\r\n",
        "import logging\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "# Evaluation\r\n",
        "results = {}\r\n",
        "if training_args.do_eval:\r\n",
        "    logger.info(\"*** Evaluate ***\")\r\n",
        "    results = trainer.evaluate()\r\n",
        "\r\n",
        "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\r\n",
        "    if trainer.is_world_process_zero():\r\n",
        "        with open(output_eval_file, \"w\") as writer:\r\n",
        "            logger.info(\"***** Eval results *****\")\r\n",
        "            for key, value in sorted(results.items()):\r\n",
        "                logger.info(f\"  {key} = {value}\")\r\n",
        "                writer.write(f\"{key} = {value}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ct9BWqHRR-9",
        "outputId": "988b23be-e0da-45df-9cb1-8a340153fb46"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('exact', 70.04969257980291),\n",
              "             ('f1', 73.22756089117455),\n",
              "             ('total', 11873),\n",
              "             ('HasAns_exact', 70.95141700404858),\n",
              "             ('HasAns_f1', 77.31626694684802),\n",
              "             ('HasAns_total', 5928),\n",
              "             ('NoAns_exact', 69.15054667788057),\n",
              "             ('NoAns_f1', 69.15054667788057),\n",
              "             ('NoAns_total', 5945),\n",
              "             ('best_exact', 70.04969257980291),\n",
              "             ('best_exact_thresh', 0.0),\n",
              "             ('best_f1', 73.2275608911745),\n",
              "             ('best_f1_thresh', 0.0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ0YBRN_YwsI"
      },
      "source": [
        "### <b> 8. Test with your own Input </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KAK-I8ZfBEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bac5a4-7922-402d-fe5e-162a86536370"
      },
      "source": [
        "index = 2708\r\n",
        "text = datasets['validation'][index]['context']\r\n",
        "question = datasets['validation'][index]['question']\r\n",
        "enforce_answer = False\r\n",
        "\r\n",
        "encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\r\n",
        "input_ids = encoding[\"input_ids\"]\r\n",
        "\r\n",
        "# default is local attention everywhere\r\n",
        "# the forward method will automatically set global attention on question tokens\r\n",
        "attention_mask = encoding[\"attention_mask\"]\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  logits = model(input_ids.cuda(), attention_mask.cuda())\r\n",
        "\r\n",
        "#start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\r\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\r\n",
        "\r\n",
        "st = 1 if enforce_answer else 0\r\n",
        "answer_tokens = all_tokens[torch.argmax(logits.start_logits[0][st:]) :torch.argmax(logits.end_logits[0][st:])+1]\r\n",
        "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\r\n",
        "\r\n",
        "print(\"Predicted Answer:\", answer)\r\n",
        "print(\"Real Answer:\", datasets['validation'][index]['answers']['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Answer: march 1974\n",
            "Real Answer: ['March 1974', 'March 1974', 'March 1974', 'March 1974.', 'March 1974']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKuny3Y5XLsk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}